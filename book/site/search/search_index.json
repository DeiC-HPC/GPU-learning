{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"directives/","title":"Directives","text":"<p>{:.code-info cuda pycuda opencl pyopencl hip} This part of the book is not relevant for you chosen environment. Please go here or change environment.</p> <p>{{#include mandelbrot-sequential-implementation.md}}</p>"},{"location":"directives/#before-we-start","title":"Before we start","text":"<p>Before we start converting the program to run on GPUs, we need to lay some groundwork. We need to understand how to make the program run on the GPU and how to get our data to and from it.</p> C++ OpenMPC++ OpenACCFortran OpenMPFortran OpenACC <p>OpenMP is a set of pragmas, which the compiler uses to optimize your code. These directives start with <code>#pragma omp</code> where <code>omp</code> is the start of the OpenACC statement.</p> <p>OpenACC is a set of pragmas, which the compiler uses to optimize your code. These directives start with <code>#pragma acc</code> where <code>acc</code> is the start of the OpenACC statement.</p> <p>OpenMP is a set of pragmas, which the compiler uses to optimize your code. These directives start with <code>$!omp</code> where <code>omp</code> is the start of the OpenACC statement.</p> <p>OpenACC is a set of pragmas, which the compiler uses to optimize your code. These directives start with <code>$!acc</code> where <code>acc</code> is the start of the OpenACC statement.</p> <p>This means that you mostly write your programs as you normally do but with some exceptions as we need to make it work on the GPU as well.</p> <p>When compiling there are several available options, but in this tutorial we will focus on GCC.</p> OpenACCOpenMP <p>In GCC you add the flag <code>-fopenacc</code> when you want the compiler to understand the OpenACC pragmas.</p> <p>In GCC you add the flag <code>-fopenmp</code> when you want the compiler to understand the OpenMP pragmas.</p> <p>When you compile for the GPU then you will also need to use <code>-fno-stack-protector</code> as these checks will not work and cause the program to crash. If you use any math libraries then you will also need to add <code>-foffload=-lm</code>.</p> <p>If you are using CUDA 11 then you also have to add <code>-foffload=\"-misa=sm_35\"</code> as the default currently is <code>sm_30</code>, which is no longer supported. Also the only two options are <code>sm_30</code> and <code>sm_35</code>.</p>"},{"location":"directives/#converting-to-directives","title":"Converting to directives","text":"<p>TODO: fix this section to be compatible with OpenMP and OpenACC in C++ and Fortran</p> <p>The program does not change much from the original when adding the pragmas but let us take a look.</p> <p>{:.code-info cpp-openacc} Before the mandelbrot function there has been added <code>#pragma acc routine</code>. This means that it should be compiled for our offload target.</p> <p>{:.code cpp-openacc} <pre><code>{{#include ../../examples/mandelbrot/cpp/openacc/openacc.cpp:mandelbrot}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code-info cpp-openmp} Around the mandelbrot function there has been added <code>#pragma omp declare target</code> and <code>#pragma omp end declare target</code>. This means that it should be compiled for our offload target.</p> <p>{:.code cpp-openmp} <pre><code>{{#include ../../examples/mandelbrot/cpp/openmp/openmp.cpp:mandelbrot}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code-info f90-openmp} Inside the mandelbrot function <code>$!acc routine</code> has been added. This means that it should be compiled for our offload target.</p> <p>{:.code f90-openmp} <pre><code>{{#include ../../examples/mandelbrot/fortran/openmp/openmp.f90:mandelbrot}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code-info f90-openacc} Inside the mandelbrot function <code>$!acc routine</code> has been added. This means that it should be compiled for our offload target.</p> <p>{:.code f90-openacc} <pre><code>{{#include ../../examples/mandelbrot/fortran/openacc/openacc.f90:mandelbrot}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code cpp-openmp} <pre><code>{{#include ../../examples/mandelbrot/cpp/openmp/openmp.cpp:loops}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code-info cpp-openacc} Before the loop we have a pragma <code>#pragma acc parallel loop collapse(2) copyin(zs[:width * height]) copyout(res[:width * height])</code> which is where we tell the compiler what we want to happen. <code>parallel loop</code> tells the compiler that the loop is completely parallel and that every part can be run by itself. The <code>collapse(2)</code> tells the compiler that it can parallelize both loops. At last we have <code>copyout(res)</code>, which is a data clause. Here we say that we want to copy the variable <code>res</code> back from the GPU.  There are also other data clauses besides <code>copyout</code>. <code>copyin</code> is for copying data to the GPU. <code>copy</code> is for copying data to and from the GPU, so data is copied to the GPU first and then back when it is done. <code>create</code> is for allocating the variable on the GPU. <code>present</code> tells the compiler that it is already there. There is also <code>deviceptr</code> which says that the variable is already on the GPU and it is containing a pointer to the device memory. This is only useful when using OpenACC together with another programming model.</p> <p>{:.code cpp-openacc} <pre><code>{{#include ../../examples/mandelbrot/cpp/openacc/openacc.cpp:loops}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code f90-openmp} <pre><code>{{#include ../../examples/mandelbrot/fortran/openmp/openmp.f90:loops}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code-info f90-openacc} Before the loop we have a pragma <code>$!acc parallel loop collapse(2) copyout(res)</code> which is where we tell the compiler what we want to happen. <code>parallel loop</code> tells the compiler that the loop is completely parallel and that every part can be run by itself. The <code>collapse(2)</code> tells the compiler that it can parallelize both loops. At last we have <code>copyout(res)</code>, which is a data clause. Here we say that we want to copy the variable <code>res</code> back from the GPU.  There are also other data clauses besides <code>copyout</code>. <code>copyin</code> is for copying data to the GPU. <code>copy</code> is for copying data to and from the GPU, so data is copied to the GPU first and then back when it is done. <code>create</code> is for allocating the variable on the GPU. <code>present</code> tells the compiler that it is already there. There is also <code>deviceptr</code> which says that the variable is already on the GPU and it is containing a pointer to the device memory. This is only useful when using OpenACC together with another programming model.</p> <p>{:.code f90-openacc} <pre><code>{{#include ../../examples/mandelbrot/fortran/openacc/openacc.f90:loops}}\n</code></pre>Run the code in Jupyter</p>"},{"location":"error-handling/","title":"Error handling","text":"<p>TODO</p>"},{"location":"exploiting-parallelism/","title":"Exploiting parallelism","text":"<p>We have now talked about the basic things of GPU programming. Now we have to put that into practice. Sometimes when you have many loops it can be difficult to figure out if they are actually parallel or partly parallel so they can be split up.</p>"},{"location":"exploiting-parallelism/#0-when-are-loops-parallel","title":"0 When are loops parallel?","text":"<p>TODO write section differently, explainations are bad and needs to be more pedagogical</p> <p>When we look at nested loops it can be difficult to spot if they are parallelisable. But there are some techniques, which can help you do so. One we can use is called direction vectors. By determining the direction of dependence in a loop we can see if a loop is parallel.</p> <p>There is exists three types of dependencies: - True dependency, also called read after write (RAW). - Anti dependency, also called write after read (WAR). - Output dependency, also called write after write (WAW).</p> <p>In a loop this is defined as what happens in the previous iteration. For example <pre><code>FOR i = 1 TO N\n    A[i] = A[i-1]\n</code></pre> This is a RAW as every iteration reads what has been written in the previous.</p> <p>To create a direction vector we use the following characters <code>=</code>, <code>&lt;</code>, and <code>&gt;</code>. <code>=</code> is used for WAW operations, <code>&lt;</code> for RAW operations, and <code>&gt;</code> for WAR operations.</p> <p>For some more advanced examples we have <pre><code>FOR i = 0 TO N\n    FOR j = 0 TO N\n        A[i,j] = A[i,j] ...\n</code></pre> Here the direction vector is <code>[=, =]</code> or WAW in both directions.</p> <p><pre><code>FOR i = 0 TO N\n    FOR j = 1 TO N\n        A[j,i] = A[j-1,i] ...\n</code></pre> Here the direction vectors is <code>[=, &lt;]</code>, where the outer loop is WAW, and the inner RAW.</p> <p><pre><code>FOR i = 0 TO N\n    FOR j = 0 TO N\n        A[i,j] = A[i-1,j+1] ...\n</code></pre> For the last example we have the direction vector <code>[&lt;, &gt;]</code>, which means the outer loop is RAW, and the inner is WAR.</p> <p>And then we have the important part about direction vectors, which is that loop in a loop is parallel if all its directions are <code>=</code> or there exist an outer loop whose direction is <code>&lt;</code>. This means we can determine parallelism from our direction vectors. Another thing we know is that direction vectors can not have <code>&gt;</code> as the first non-<code>=</code> symbol. This would lead us to depend on something that we have yet to calculate.</p>"},{"location":"exploiting-parallelism/#loop-interchange","title":"Loop interchange","text":"<p>To exploit even more parallelism in our code, we can use loop interchange. By doing loop interchange we can also make sure that we get coalesced memory access, which we have seen earlier can bring us performance gains.</p> <p>Loop interchange is allowed if and only if it does not result in a <code>&gt;</code> direction as the leftmost non-<code>=</code> direction.</p>"},{"location":"exploiting-parallelism/#1-loop-interchange-example","title":"1 Loop interchange example","text":"<p>TODO: make WAR example with result array</p>"},{"location":"further-reading/","title":"Further reading","text":"<p>HIP - Introduction to Programming with ROCm</p> <p>CUDA - CUDA C++ best practices</p> <p>OpenCL</p> <p>OpenMP - OpenMP Syntax Reference Guide</p> <p>OpenACC - OpenACC - Get Started - OpenACC Programming and Best Practices Guide</p>"},{"location":"getting-started/","title":"Getting started","text":"<p>In this part we are going to show you how to run a program on a GPU, this is done using an example program, which is converted in a few steps to run on the GPU. As an example program we are going to look at calculating the Mandelbrot set.</p>"},{"location":"getting-started/#before-we-start","title":"Before we start","text":"<p>Before we start converting the program to run on GPUs, we need to lay some groundwork. We need to understand how to make the program run on the GPU and how to get our data to and from it.</p> <p>There are a number of different approaches we can choose from.</p> <p>CUDA: CUDA code is a variant of <code>C++</code> with some extra syntax. CUDA code typically uses the file-extension <code>.cu</code>. The main difference between CUDA code and <code>C++</code>, is that you need to declare whether something should be runnable on the CPU, GPU or both. It has special syntax for invoking GPU code from the CPU. To run CUDA code, you can either write entire programs in CUDA and compile them directly using the NVIDIA CUDA Compiler (<code>nvcc</code>), or use the PyCUDA library to invoke CUDA code from inside Python.</p> <p>HIP: HIP code is a near 1:1 in syntax to CUDA and is created to give the same performance, but just compile to both AMD and NVIDIA GPUs. To compile HIP code, you will need the AMD ROCm environment.</p> <p>OpenMP/OpenACC: OpenMP and OpenACC are both a collection of pragma-based annotations for <code>C</code>, <code>C++</code> and <code>Fortran</code>. These annotations specify for instance how to parallelize code or when to copy memory. Since these annotations does not affect the syntax of the underlying program, it is mostly possible to compile OpenMP/OpenACC programs while ignoring the annotations \u00ad\u2014 however doing so will often change the behavior of the program. OpenMP and OpenACC are very similar and share a lot of history. The main difference between them is that OpenACC was originally designed for GPU parallelization, while OpenMP was designed for CPU parallelization and got GPU support added at a later time.</p> <p>OpenCL: TODO: Write something about OpenCL.</p> <p>This guide has been written in multiple versions, depending on which platform you want to learn. You can change which version you are viewing to using the drop-down menu at the top. Try changing it now and see how the text below changes:</p> CUDAHIPPyCuda <p>This text is specific to the CUDA guide.</p> <p>This text is specific to the HIP guide.</p> <p>This text is specific to the PyCUDA guide.</p> <p>{:.code-info pyopencl} This text is specific to the PyOpenCL guide.</p> <p>{:.code-info cpp-openmp} This text is specific to the OpenMP guide (in C++).</p> <p>{:.code-info f90-openmp} This text is specific to the OpenMP guide (in Fortran).</p> <p>{:.code-info cpp-openacc} This text is specific to the OpenACC guide (in C++).</p> <p>{:.code-info f90-openacc} This text is specific to the OpenACC guide (in Fortran).</p>"},{"location":"getting-started/#1-choosing-the-right-compiler","title":"1 Choosing the right compiler","text":"<p>Choosing the right environment will most likely be based on what language your current software project is using. But we will still outline some advantages and disadvantages of each to help you choose, if you have multiple options.</p>"},{"location":"getting-started/#cuda","title":"CUDA","text":"<p>Advantages - Native performance</p> <p>Disadvantages - Difficult to get optimal performance - Only works for NVIDIA devices</p>"},{"location":"getting-started/#hip","title":"HIP","text":"<p>Advantages - Native performance - Works with multiple vendors</p> <p>Disadvantages: - Difficult to get optimal performance - Performs worse than CUDA on NVIDIA GPUs in some cases</p>"},{"location":"getting-started/#opencl","title":"OpenCL","text":"<p>Advantages - Native performance - Works with multiple vendors</p> <p>Disadvantages - Difficult to get optimal performance - Complicated programming model compared to CUDA - Performance is a bit worse compared to HIP and CUDA</p>"},{"location":"getting-started/#openmp","title":"OpenMP","text":"<p>Advantages - Easy to get started with - Code can be run on both multi core CPU and GPU</p> <p>Disadvantages - Performance based on compiler implementation</p>"},{"location":"getting-started/#openacc","title":"OpenACC","text":"<p>Advantages - Easy to get started with</p> <p>Disadvantages - Performance based on compiler implementation</p> <p>If you have chosen CUDA or OpenCL, go to the subchapter native, else go to directives.</p>"},{"location":"introduction/","title":"Introduction","text":"<p>This book contains material for learning GPU programming in various programming environments. It is meant for people who are confident in solving their own problems on a CPU but needs more computing capabilities.</p> <p>It is important to note before we start that not all GPUs are equal. So you will not necessarily see the same performance benefits by implementing your code on different GPUs. Newer GPUs are also more capable and include many functionalities that is not available on older devices, for example tensor cores for machine learning purposes.</p>"},{"location":"introduction/#before-we-begin","title":"Before we begin","text":"<p>Before we go ahead and start programming GPUs in any of the programming languages, I will give you a short introduction to how they work and are different than general purpose CPUs. This information should help you understand which problems are suitable to be run on a GPU and something something...</p> <p>General purpose CPUs are what we call SISD (Single Instruction Stream, Single Data Stream) which means that every instruction on the CPU is only working on a single item at a time. This is not entirely true with CPUs implementing AVX instructions, but let us not get into that here.</p> <p>A GPU is instead what we call SIMD (Single Instruction, Multiple Data). This means that everything we do applies to multiple data items at once.</p> <p>Let us look at an example: <pre><code>i = some_index\na[i] = 42\n</code></pre></p> <p>On a CPU we would set a specific index in <code>a</code> and to overwrite the whole array we would need a loop of some sort to overwrite all values.</p> <p>On a GPU the variable <code>i</code> would have a different value for each core, meaning that we could set all values in <code>a</code> with just one line of code instead of using a loop.</p> <p>Due to the fact that we are running the same instructions on multiple data entities at the same time, we will have to think differently about control flow structures we normally use.</p> <p>If your code contains an if-else statement both the if and else part will be run for all data, but only the part which satisfies the condition will be active. This means that code with big if-else blocks will be ineffective to run on a GPU.</p>"},{"location":"introduction/#compilers-used","title":"Compilers used","text":"<p>Also it is worth noting that the examples in this book are using the following compilers:</p> <ul> <li>HIP, ROCm 4.5</li> <li>Cuda, NVCC 11</li> <li>OpenMP and OpenACC, GCC 10.2.0 with NVPTX offloading.</li> <li>Python Cuda, PyCuda 2019.1.2</li> <li>Python OpenCL, PyOpenCL 2020.2.2</li> </ul> <p>If you do not have these compilers on your system and you are not comfortable with compiling them yourself then we have made a package for you. They require that you use a system compatible with the nix environment.</p> <p>TODO: write how to get them up and running on peoples own system.</p>"},{"location":"latency-hiding/","title":"Latency hiding","text":"<p>TODO</p>"},{"location":"mandelbrot-sequential-implementation/","title":"Mandelbrot sequential implementation","text":""},{"location":"mandelbrot-sequential-implementation/#sequential-implementation","title":"Sequential implementation","text":"<p>To calculate the Mandelbrot set, we map each point \\(c\\) in the complex plane to a function \\( f_c(z) = z^2 + c \\). The Mandelbrot, is the set of points such that iterated application of \\(f_c\\) remains bounded forever, i.e. \\(|f_c(f_c(\\dots f_c(0) \\dots))|\\) must not diverge to infinity.</p> <p>When visualizing the Mandelbrot, one is also interested in how quickly this expression grows beyond the circle bounded by \\(|z|&lt;2\\).</p> <p>The sequential version contains a function called <code>mandelbrot</code>, which is all the logic we need to calculate the Mandelbrot set.</p> <p>{:.code cuda cpp-openmp cpp-openacc} <pre><code>{!../../examples/mandelbrot/cpp/reference-implementation.cpp!}\n</code></pre>Run the code in Jupyter</p> <p>{:.code f90-openmp f90-openacc} <pre><code>{{#include ../../examples/mandelbrot/fortran/reference-implementation.f90:mandelbrot}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pycuda pyopencl} <pre><code>{{#include ../../examples/mandelbrot/python/reference-implementation.py:mandelbrot}}\n</code></pre>Run the code in Jupyter</p> <p>It takes a complex number <code>z</code> and a maximum number of iterations to be run.</p> <p>To setup the function we have a lot of variables with default values defining width and height of the image we are generating, how many iterations should at most be run in the <code>mandelbrot</code> function, and which area of the fractal should be shown (default is everything).</p> <p>Then we have two nested loops creating a complex number in the range of the minimum and maximum values and then calculating the mandelbrot function for each of these numbers.</p> <p>The data is then written to disk so we can visualize it and see the mandelbrot.</p>"},{"location":"memory-coalescing/","title":"Memory Coalescing","text":"<p>In this part we will talk about memory coalescence. We will talk about what it is and why it is important. We will also showcase a program, where we will see how it should be done and how it should not be done.</p>"},{"location":"memory-coalescing/#what-is-it","title":"What is it?","text":"<p>On a GPU we have three layers of memory: - Global - Shared - Local (registers)</p> <p>When we access global memory on a gpu, we access multiple elements at the same time. This is important to keep in mind, when programming, because access to global memory is slow. So we need to utilise that we are accessing multiple elements at the same time. Therefore we need adjacent threads on the GPU to access adjacent memory in order to gain maximum performance.</p>"},{"location":"memory-coalescing/#matrix-addition","title":"Matrix addition","text":"<p>We will be looking at matrix addition, but for teaching purposes we will only parallelise one dimension. We will show the differences in parallelising each dimension and describe why there is a difference.</p>"},{"location":"memory-coalescing/#parallelising-the-outer-loop","title":"Parallelising the outer loop","text":"<p>When programming a CPU the correct thing to do would be to parallelise the outer loop, because we would then get cache coherency. So this is what we have done in the first part. This is not optimal on a GPU, because when we access memory, we get multiple elements at the same time as described earlier. When parallelising the outer loop, every thread in the thread block will read their section of memory, which requires multiple reads of global memory.</p> <p></p> <p>{:.code cuda} <pre><code>{{#include ../../examples/matrixaddition/cpp/cuda/naive.cu:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pycuda} <pre><code>{{#include ../../examples/matrixaddition/python/cuda/naive.py:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pyopencl} <pre><code>{{#include ../../examples/matrixaddition/python/opencl/naive.py:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code cpp-openmp} <pre><code>{{#include ../../examples/matrixaddition/cpp/openmp/naive.cpp:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code cpp-openacc} <pre><code>{{#include ../../examples/matrixaddition/cpp/openacc/naive.cpp:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code f90-openmp} <pre><code>{{#include ../../examples/matrixaddition/fortran/openmp/naive.f90:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code f90-openacc} <pre><code>{{#include ../../examples/matrixaddition/fortran/openacc/naive.f90:matrixaddition}}\n</code></pre>Run the code in Jupyter</p>"},{"location":"memory-coalescing/#parallelising-the-inner-loop","title":"Parallelising the inner loop","text":"<p>To fix the error in the previous version, we instead parallelise the inner loop. This means when we are reading data from global memory, then every data point is given to a thread and no data is fetched without being assigned to a thread.</p> <p></p> <p>{:.code cuda} <pre><code>{{#include ../../examples/matrixaddition/cpp/cuda/optimized.cu:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pycuda} <pre><code>{{#include ../../examples/matrixaddition/python/cuda/optimized.py:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pyopencl} <pre><code>{{#include ../../examples/matrixaddition/python/opencl/optimized.py:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code cpp-openmp} <pre><code>{{#include ../../examples/matrixaddition/cpp/openmp/optimized.cpp:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code cpp-openacc} <pre><code>{{#include ../../examples/matrixaddition/cpp/openacc/optimized.cpp:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code f90-openmp} <pre><code>{{#include ../../examples/matrixaddition/fortran/openmp/optimized.f90:matrixaddition}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code f90-openacc} <pre><code>{{#include ../../examples/matrixaddition/fortran/openacc/optimized.f90:matrixaddition}}\n</code></pre>Run the code in Jupyter</p>"},{"location":"memory-management/","title":"Memory management","text":"<p>When using a GPU one of the most important things to do right besides memory coalescence is memory management. Moving data to and from the GPU takes time, which could have been used on calculations. Therefore will we in this section look into how we can better manage our memory transactions.</p>"},{"location":"memory-management/#in-general","title":"In general","text":"<p>In our previous examples we have moved data between CPU and GPU when doing our loops. The examples have also been fairly simple with only one loop. When working with bigger programs with many different loops.</p> <p>{:.code-info cpp-openmp} Here we can either use <code>#pragma omp target data</code>, where we use curly brackets to create the scope for the data, or <code>#pragma omp target enter data</code> and <code>#pragma omp target exit data</code>.</p> <p>{:.code-info cpp-openacc} Here we can either use <code>#pragma acc data</code>, where we use curly brackets to create the scope for the data, or <code>#pragma acc enter data</code> and <code>#pragma acc exit data</code>.</p> <p>{:.code-info f90-openmp} Here we can either use <code>!$omp target data</code> and <code>!$omp end target data</code> around your code or <code>!$omp target enter data</code> and <code>!$omp target exit data</code>.</p> <p>{:.code-info f90-openacc} Here we can either use <code>!$acc data</code> and <code>!$omp end data</code> around your code or <code>!$acc enter data</code> and <code>!$acc exit data</code>.</p>"},{"location":"memory-management/#map-reduce-example","title":"Map reduce example","text":"<p>To show how much a difference it makes and how to use data transfer, we will now look at a map reduce example. Map reduce refers to two operations normally used in functional programming. A map is where you do the same operation, for example adding two to each element, over every element in a list. Reduce is then taking a list and then reducing it to a single element. This could for example be getting the sum a list. If you're doing a reduction loop then you have to add a <code>reduction</code> clause to your pragma. Inside the reduction clause you have to set a reduction operator and then a number of variables.</p> <p>{:.code-info cpp-openacc cpp-openmp} The possible reduction operators are <code>+</code>, <code>*</code>, <code>max</code>, <code>min</code>, <code>&amp;</code>, <code>|</code>, <code>^</code>, <code>&amp;&amp;</code>, and <code>||</code>.</p> <p>{:.code-info f90-openacc f90-openmp} The possible reduction operators are <code>+</code>, <code>*</code>, <code>max</code>, <code>min</code>, <code>iand</code>, <code>ior</code>, <code>ieor</code>, <code>.and.</code>, <code>.or</code>, <code>.eqv.</code>, and <code>.neqv</code>.</p> <p>The two programs are based around two loops the first being a map and the second being a reduce. In the not optimized program we copy variables in both loops.</p> <p>{:.code f90-openacc} <pre><code>{{#include ../../examples/mapreduce/fortran/openacc/naive.f90:mapreduce}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code f90-openmp} <pre><code>{{#include ../../examples/mapreduce/fortran/openmp/naive.f90:mapreduce}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code cpp-openmp} <pre><code>{{#include ../../examples/mapreduce/cpp/openmp/naive.cpp:mapreduce}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code cpp-openacc} <pre><code>{{#include ../../examples/mapreduce/cpp/openacc/naive.cpp:mapreduce}}\n</code></pre>Run the code in Jupyter</p> <p>In the optimized version we put the code into a data region and create the <code>elements</code> array on the GPU and then do our calculations so it is never copied. The only variable that is copied is the <code>res</code>. As copying variables and arrays between CPU and GPU is an expensive operation then the goal is to limit that.</p> <p>{:.code f90-openacc} <pre><code>{{#include ../../examples/mapreduce/fortran/openacc/optimized.f90:mapreduce}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code f90-openmp} <pre><code>{{#include ../../examples/mapreduce/fortran/openmp/optimized.f90:mapreduce}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code cpp-openmp} <pre><code>{{#include ../../examples/mapreduce/cpp/openmp/optimized.cpp:mapreduce}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code cpp-openacc} <pre><code>{{#include ../../examples/mapreduce/cpp/openacc/optimized.cpp:mapreduce}}\n</code></pre>Run the code in Jupyter</p>"},{"location":"memory/","title":"Memory","text":""},{"location":"mkdocs/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"mkdocs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"mkdocs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> Unix, PowershellWindowsFortranC++ <pre><code>docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build\n</code></pre> <pre><code>docker run --rm -it -v \"%cd%\":/docs squidfunk/mkdocs-material build\n</code></pre> <pre><code>a = 2\n</code></pre> <pre><code>int main() {\nreturn 0;\n}\n</code></pre>"},{"location":"native/","title":"Native","text":"<p>{:.code-info cpp-openmp cpp-openacc f90-openmp f90-openacc} This part of the book is not relevant for you chosen environment. Please go here or change environment.</p> <p>\\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\), \\(p(x|y) = \\frac{p(y|x)p(x)}{p(y)}\\) Sequential implementation</p> <p>To calculate the Mandelbrot set, we map each point \\(c\\) in the complex plane to a function \\( f_c(z) = z^2 + c \\). The Mandelbrot, is the set of points such that iterated application of \\(f_c\\) remains bounded forever, i.e. \\(|f_c(f_c(\\dots f_c(0) \\dots))|\\) must not diverge to infinity.</p> <p>When visualizing the Mandelbrot, one is also interested in how quickly this expression grows beyond the circle bounded by \\(|z|&lt;2\\).</p> <p>The sequential version contains a function called <code>mandelbrot</code>, which is all the logic we need to calculate the Mandelbrot set.</p> <p>{:.code cuda cpp-openmp cpp-openacc} <pre><code>{!../../examples/mandelbrot/cpp/reference-implementation.cpp!}\n</code></pre>Run the code in Jupyter</p> <p>{:.code f90-openmp f90-openacc} <pre><code>{{#include ../../examples/mandelbrot/fortran/reference-implementation.f90:mandelbrot}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pycuda pyopencl} <pre><code>{{#include ../../examples/mandelbrot/python/reference-implementation.py:mandelbrot}}\n</code></pre>Run the code in Jupyter</p> <p>It takes a complex number <code>z</code> and a maximum number of iterations to be run.</p> <p>To setup the function we have a lot of variables with default values defining width and height of the image we are generating, how many iterations should at most be run in the <code>mandelbrot</code> function, and which area of the fractal should be shown (default is everything).</p> <p>Then we have two nested loops creating a complex number in the range of the minimum and maximum values and then calculating the mandelbrot function for each of these numbers.</p> <p>The data is then written to disk so we can visualize it and see the mandelbrot.</p>"},{"location":"native/#how-do-we-then-use-the-gpu","title":"How do we then use the GPU?","text":"<p>Writing TODO: Some intro text</p> <p>Kernels, as the functions running on GPUs are called, have <code>__global__</code> before their return type and name. The return type will always be <code>void</code> because these functions does not return anything. Instead the data is copied to and from the GPU.</p> <pre><code>__global__ void someKernel(\nconst float *readOnlyArgument,\nfloat *writableArgument,\nfloat someConstant)\n{\nint i = blockIdx.x*blockDim.x+threadIdx.x;\n}\n</code></pre> <p>With the code above we see three variables, <code>blockIdx</code>, <code>blockDim</code>, and <code>threadIdx</code>, that we have not defined. These will be instantiated when we are running and tell us know where we are running. When running our code, it is run in a grid of thread block. Each thread block can have up to 1024 threads and must be a power of 2.</p> CUDAHIP <p>So how do we allocate memory on the GPU and copy data to and from it? There are two ways that you can do this. Firstly you can use <code>cudaMalloc</code>, where you have control and choose when to copy to and from the GPU.</p> <pre><code>float* numbers = (float*)malloc(n*sizeof(float));\nfloat* numbers_device;\ncudaMalloc((void**)&amp;numbers_device, n*sizeof(float));\n// Copying to the device\ncudaMemcpy(numbers_device, numbers, n*sizeof(float), cudaMemcpyHostToDevice);\n// Copying from the device\ncudaMemcpy(numbers, numbers_device, n*sizeof(float), cudaMemcpyDeviceToHost);\n</code></pre> <p>It is also possible to use <code>cudaMallocManaged</code>, where copying will be done for you. This can lead to worse performance.</p> <pre><code>float* someMem;\ncudaMallocManaged(&amp;someMem, n*sizeof(float));\n</code></pre> <p>So how do we allocate memory on the GPU and copy data to and from it? There are two ways that you can do this. Firstly you can use <code>hipMalloc</code>, where you have control and choose when to copy to and from the GPU.</p> <p><pre><code>float* numbers = (float*)malloc(n*sizeof(float));\nfloat* numbers_device;\nhipMalloc((void**)&amp;numbers_device, n*sizeof(float));\n// Copying to the device\nhipMemcpy(numbers_device, numbers, n*sizeof(float), hipMemcpyHostToDevice);\n// Copying from the device\nhipMemcpy(numbers, numbers_device, n*sizeof(float), hipMemcpyDeviceToHost);\n</code></pre> It is also possible to use <code>hipMallocManaged</code>, where copying will be done for you. This can lead to worse performance. Given it is not supported on all GPUs, we should also include a compatibility check before making the calls.</p> <pre><code>int p_gpuDevice = 0; // GPU device number\nint managed_memory = 0;\nhipGetDeviceAttribute(&amp;managed_memory, hipDeviceAttributeManagedMemory,p_gpuDevice));\nfloat* someMem;\nif (!managed_memory) {\n// Return an error message\n} else {\nhipMallocManaged(&amp;someMem, n*sizeof(float));\n}\n</code></pre> <p>When calling our kernel we need to define the dimensions of our kernel and thread blocks. As said earlier the have up to 1024 threads and each dimension must be a power of 2. The grid is then defined as the number of thread blocks we want in each dimension.</p> <pre><code>dim3 grid(n,m,1);\ndim3 block(16,16,1);\nsomeKernel&lt;&lt;&lt; grid, block &gt;&gt;&gt;(readable, writable, 5.0f);\n</code></pre>"},{"location":"native/#naive-implementation","title":"Na\u00efve implementation","text":"CUDAHIP <p>In this version we have taken the na\u00efve approach and done a direct translation of the program. To use the library for complex arithmetic, we start by writing <code>#include \"cuComplex.h\"</code>. This enables us to use the type <code>cuFloatComplex</code>, and the functions <code>cuCmulf</code> (multiplication of complex numbers) and <code>cuCaddf</code> (addition of complex numbers). Underneath the <code>cuFloatComplex</code> type is a vector type called <code>float2</code>, a 2D floating point vector. CUDA has multiple types, which support vector types, those are char, uchar, short, ushort, int, uint, long, ulong, longlong, ulonglong, float, and double. The length of the vector types can be 2, 3, and 4.</p> <p>In this version we have taken the na\u00efve approach and done a direct translation of the program. To use the library for complex arithmetic, we start by writing <code>#include \"hip/hip_complex.h\"</code>. This enables us to use the type <code>hipFloatComplex</code>, and the functions <code>hipCmulf</code> (multiplication of complex numbers) and <code>hipCaddf</code> (addition of complex numbers). Underneath the <code>hipFloatComplex</code> type is a vector type called <code>float2</code>, a 2D floating point vector. HIP has multiple types, which support vector types, those are char, uchar, short, ushort, int, uint, long, ulong, longlong, ulonglong, float, and double. The length of the vector types can be 2, 3, and 4.</p> <p>The only translation we have done in this version is the <code>mandelbrot</code> function and the complex arithmetic, which means all data is still generated and sent from the host. But looking at the function we see, that we have to send the width and height to the function is because we are running in thread blocks, as described earlier. We could end up out of bounds of our array, which we do not want and therefore we have this <code>if</code>-statement.</p> CUDAHIP <p><pre><code>__global__ void mandelbrot(\nconst cuFloatComplex *zs,\nint *res,\nushort width,\nushort height,\nushort max_iterations) {\nint x = blockIdx.x * blockDim.x + threadIdx.x;\nint y = blockIdx.y * blockDim.y + threadIdx.y;\nif (x &gt;= width || y &gt;= height) {\nreturn;\n}\ncuFloatComplex z = zs[y * width + x];\ncuFloatComplex c = z;\nint i;\nfor (i = 0; i &lt; max_iterations; i++) {\nif (z.x * z.x + z.y * z.y &lt;= 4.0f) {\nz = cuCmulf(z, z);\nz = cuCaddf(z, c);\n} else {\nbreak;\n}\n}\nres[y * width + x] = i;\n}\n</code></pre> Run the code in Jupyter</p> <pre><code>__global__ void mandelbrot(\nconst hipFloatComplex *zs,\nint *res,\nushort width,\nushort height,\nushort max_iterations) {\nint x = blockIdx.x * blockDim.x + threadIdx.x;\nint y = blockIdx.y * blockDim.y + threadIdx.y;\nif (x &gt;= width || y &gt;= height) {\nreturn;\n}\nhipFloatComplex z = zs[y * width + x];\nhipFloatComplex c = z;\nint i;\nfor (i = 0; i &lt; max_iterations; i++) {\nif (z.x * z.x + z.y * z.y &lt;= 4.0f) {\nz = hipCmulf(z, z);\nz = hipCaddf(z, c);\n} else {\nbreak;\n}\n}\nres[y * width + x] = i;\n</code></pre>"},{"location":"native/#less-transfer-implementation","title":"Less transfer implementation","text":"<p>Transferring data to and from the GPU takes time, which in turn makes our calculations slower. So we should try to limit how much data we move around. In the na\u00efve version we generate our data on the host and create a 1000 by 1000 matrix, which is then transfered to the GPU. But we can be smarter than that. By sending our lists of real and imaginary parts, we can then combine them on the GPU saving both time and space, because we already have the coordinates of from our two global ids.</p> CUDAHIP <pre><code>__global__ void mandelbrot(\nconst float *re,\nconst float *im,\nint *res,\nushort width,\nushort height,\nushort max_iterations) {\nint x = blockIdx.x * blockDim.x + threadIdx.x;\nint y = blockIdx.y * blockDim.y + threadIdx.y;\nif (x &gt;= width || y &gt;= height) {\nreturn;\n}\ncuFloatComplex z = make_cuFloatComplex(re[x], im[y]);\ncuFloatComplex c = z;\nint i;\nfor (i = 0; i &lt; max_iterations; i++) {\nif (z.x * z.x + z.y * z.y &lt;= 4.0f) {\nz = cuCmulf(z, z);\nz = cuCaddf(z, c);\n} else {\nbreak;\n}\n}\nres[y * width + x] = i;\n}\n</code></pre> <pre><code>__global__ void mandelbrot(\nconst float *re,\nconst float *im,\nint *res,\nushort width,\nushort height,\nushort max_iterations) {\nint x = blockIdx.x * blockDim.x + threadIdx.x;\nint y = blockIdx.y * blockDim.y + threadIdx.y;\nif (x &gt;= width || y &gt;= height) {\nreturn;\n}\nhipFloatComplex z = make_hipFloatComplex(re[x], im[y]);\nhipFloatComplex c = z;\nint i;\nfor (i = 0; i &lt; max_iterations; i++) {\nif (z.x * z.x + z.y * z.y &lt;= 4.0f) {\nz = hipCmulf(z, z);\nz = hipCaddf(z, c);\n} else {\nbreak;\n}\n}\nres[y * width + x] = i;\n}\n</code></pre>"},{"location":"native/#gpu-only-implementation","title":"GPU only implementation","text":"<p>But we can also eliminate the need for transfering data to GPU completely by letting the GPU do it. This will reduce total computation time considerably, especially when calculating with a higher resolution. Of course we still need to transfer the result array from the GPU, which is the majority of our data transfer, but reducing data transfer should be a priority.</p> CUDAHIP <pre><code>__global__ void mandelbrot(\nint *res,\nushort width,\nushort height,\nfloat xmin,\nfloat xdelta,\nfloat ymin,\nfloat ydelta,\nushort max_iterations) {\nint x = blockIdx.x * blockDim.x + threadIdx.x;\nint y = blockIdx.y * blockDim.y + threadIdx.y;\nif (x &gt;= width || y &gt;= height) {\nreturn;\n}\ncuFloatComplex z = make_cuFloatComplex(xmin + x * xdelta, ymin + y * ydelta);\ncuFloatComplex c = z;\nint i;\nfor (i = 0; i &lt; max_iterations; i++) {\nif (z.x * z.x + z.y * z.y &lt;= 4.0f) {\nz = cuCmulf(z, z);\nz = cuCaddf(z, c);\n} else {\nbreak;\n}\n}\nres[y * width + x] = i;\n}\n</code></pre> <pre><code>__global__ void mandelbrot(\nint *res,\nushort width,\nushort height,\nfloat xmin,\nfloat xdelta,\nfloat ymin,\nfloat ydelta,\nint max_iterations) {\nsize_t x = blockIdx.x * blockDim.x + threadIdx.x;\nsize_t y = blockIdx.y * blockDim.y + threadIdx.y;\nif (x &gt;= width || y &gt;= height) {\nreturn;\n}\nhipFloatComplex z = make_hipFloatComplex(xmin + x * xdelta, ymin + y * ydelta);\nhipFloatComplex c = z;\nint i;\nfor (i = 0; i &lt; max_iterations; i++) {\nif (z.x * z.x + z.y * z.y &lt;= 4.0f) {\nz = hipCmulf(z, z);\nz = hipCaddf(z, c);\n} else {\nbreak;\n}\n}\nres[y * width + x] = i;\n}\n</code></pre>"},{"location":"shared-memory/","title":"Shared Memory","text":"<p>{:.code-info cpp-openmp f90-openmp cpp-openacc f90-openacc} This concept is not included in this environment.</p> <p>In this part we are going to talk about shared memory and how we can use it to improve performance.</p>"},{"location":"shared-memory/#in-general","title":"In general","text":"<p>Shared memory is a small portion, typically in the order of tens of kilobytes, of memory connected to each thread block, which is controllable by the user. It is used to lessen the amount of reads and writes to global memory as the latency is around 100 times lower. Also in cases where you have many local variables, it can also be an advantage to use shared memory as they could be pushed to global memory.</p> <p>To use shared memory, you have to mark your variable with <code>__shared__</code>, like so <code>__shared__ int array[10][10];</code>. Shared memory can also be allocated dynamically using the <code>extern</code> keyword. But you then have to add an extra argument, when running the kernel to define how much shared memory you need. This is done using a named argument called <code>shared</code>, where you define how many bytes of shared memory you need.</p>"},{"location":"shared-memory/#gaussian-blur","title":"Gaussian blur","text":"<p>In this section we will be looking at gaussian blur. It is a problem where all elements are accessed many times in memory. Our first will just be the na\u00efve one, where we do not use shared memory.</p> <p>{:.code cuda} <pre><code>{{#include ../../examples/blur/cpp/cuda/naive.cu:gaussianblur}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pycuda} <pre><code>{{#include ../../examples/blur/python/cuda/naive.py:gaussianblur}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pyopencl} <pre><code>{{#include ../../examples/blur/python/opencl/naive.py:gaussianblur}}\n</code></pre> Run the code in Jupyter</p> <p>Here we can see that every thread accesses many elements around itself depending on <code>FILTER_SIZE</code>, which is <code>21</code> in our example.</p>"},{"location":"shared-memory/#shared-memory-implementation","title":"Shared Memory implementation","text":"<p>To reduce accesses to global memory, we will use shared memory. We will use the shared memory to save the part of global memory, which is read by the thread block.</p> <p>{:.code-info pycuda cuda} Before we go on, we have to introduce barriers. Barriers are a way to ensure that all threads, within a thread block, have reached a specific point before any can continue. This is useful especially when using larger kernels or shared memory, where we can make sure that every thread in the thread block has come beyond a specific point. In CUDA we can use a barrier by calling the <code>__syncthreads()</code> function. It is also important to note, that all code must pass the same barrier at the same time. Using barriers in a different way will result in undefined behaviour.</p> <p>{:.code-info pyopencl} To help us do this, we have to look closer at some features in OpenCL. Firstly we need to understand barriers. A barrier is a way to make sure that all threads are synchronised. It works by stopping threads continuing until all threads within the thread block have reached the barrier. In OpenCL, we can use barriers in two different scenarios by changing the argument when calling <code>barrier</code> function. <code>CLK_LOCAL_MEM_FENCE</code> waits until shared memory has been flushed and <code>CLK_GLOBAL_MEM_FENCE</code> waits until global memory has been flushed. It is also important to note, that all code must pass the same barrier at the same time. Using barriers in a different way will result in undefined behaviour.</p> <p> </p> <p>{:.code-info pyopencl} We also need an additional functions <code>get_local_id</code>. This functions works like <code>get_global_id</code>. <code>get_local_id</code> gets the current index in the thread block, we are working in. We need this to take advantage shared memory, because we need to know</p> <p>{:.code cuda} <pre><code>{{#include ../../examples/blur/cpp/cuda/shared_memory.cu:gaussianblur}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pycuda} <pre><code>{{#include ../../examples/blur/python/cuda/shared_memory.py:gaussianblur}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pyopencl} <pre><code>{{#include ../../examples/blur/python/opencl/shared_memory.py:gaussianblur}}\n</code></pre>Run the code in Jupyter</p>"},{"location":"shared-memory/#dynamically-allocated-shared-memory-implementation","title":"Dynamically allocated shared memory implementation","text":"<p>{:.code-info pyopencl} This feature does not exist in OpenCL.</p> <p>In this implementation we will use dynamically allocated shared memory instead of allocating it directly in the kernel. It does not yield any specific performance benefit to dynamically allocate shared memory. But it will make it possible to use it for multiple purposes or with changing block sizes.</p> <p>{:.code-info cuda} We will also need to change the way we call our kernel by adding a third argument, defining the number of bytes needed, to the brackets in the function call.</p> <p>{:.code cuda} <pre><code>{{#include ../../examples/blur/cpp/cuda/dynamic_shared_memory.cu:call}}\n</code></pre></p> <p>{:.code-info pycuda} We will also need to change the way we call our kernel by adding the argument <code>shared</code>, defining the number of bytes needed, to the function call.</p> <p>{:.code pycuda} <pre><code>{{#include ../../examples/blur/python/cuda/dynamic_shared_memory.py:call}}\n</code></pre></p> <p>{:.code cuda} <pre><code>{{#include ../../examples/blur/cpp/cuda/dynamic_shared_memory.cu:gaussianblur}}\n</code></pre>Run the code in Jupyter</p> <p>{:.code pycuda} <pre><code>{{#include ../../examples/blur/python/cuda/dynamic_shared_memory.py:gaussianblur}}\n</code></pre>Run the code in Jupyter</p>"}]}