{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared Memory\n",
    "=============\n",
    "In this part we are going to talk about shared memory and how we can use it to\n",
    "improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 In general\n",
    "------------\n",
    "Shared memory is a small portion of memory connected to each thread block, which\n",
    "is controllable by the user. It is used to lessen the amount of reads and writes\n",
    "to global memory as the latency is around 100 times lower. Also in cases where\n",
    "you have many local variables, it can also be an advantage to use shared memory\n",
    "as they could be pushed to global memory.\n",
    "\n",
    "To use shared memory, you have to mark your variable with `__shared__`, like so\n",
    "`__shared__ int array[10][10];`. Shared memory can also be allocated dynamically\n",
    "using the `extern` keyword. But you then have to add an extra argument, when\n",
    "running the kernel to define how much shared memory you need. This is done using\n",
    "a named argument called `shared`, where you define how many bytes of shared\n",
    "memory you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Matrix transposition\n",
    "----------------------\n",
    "In this section we will be looking at matrix transposition. It is a problem\n",
    "where we will get problems with memory coalescing without using shared memory.\n",
    "Our first implementation will just be the na√Øve one, where we will transpose\n",
    "directly.\n",
    "\n",
    "```c++\n",
    "__global__ void matrixtranspose(\n",
    "    const int *A,\n",
    "    int *trA,\n",
    "    ushort colsA,\n",
    "    ushort rowsA)\n",
    "{\n",
    "    int i = blockIdx.x*T + threadIdx.x;\n",
    "    int j = blockIdx.y*T + threadIdx.y;\n",
    "    if( j < colsA && i < rowsA ) {\n",
    "        trA[j * rowsA + i] = A[i * colsA + j];\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "As we can see, we will not get a coalesced memory access when writing to global\n",
    "memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        0     25000     50000 ... 624925000 624950000 624975000]\n",
      " [        1     25001     50001 ... 624925001 624950001 624975001]\n",
      " [        2     25002     50002 ... 624925002 624950002 624975002]\n",
      " ...\n",
      " [    24997     49997     74997 ... 624949997 624974997 624999997]\n",
      " [    24998     49998     74998 ... 624949998 624974998 624999998]\n",
      " [    24999     49999     74999 ... 624949999 624974999 624999999]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import math\n",
    "import time\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "        __global__ void matrixtranspose(\n",
    "            const int *A,\n",
    "            int *trA,\n",
    "            ushort colsA,\n",
    "            ushort rowsA)\n",
    "        {\n",
    "            int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "            int j = blockIdx.y*blockDim.y + threadIdx.y;\n",
    "            if( j < colsA && i < rowsA ) {\n",
    "                trA[j * rowsA + i] = A[i * colsA + j];\n",
    "            }\n",
    "        }\n",
    "        \"\"\")\n",
    "\n",
    "width = 25000\n",
    "height = 25000\n",
    "\n",
    "a = np.arange(height * width).astype(np.int32)\n",
    "a.shape = (height, width)\n",
    "\n",
    "trA = np.empty((width, height)).astype(np.int32)\n",
    "\n",
    "dim_size = 32\n",
    "block_size = (dim_size,dim_size,1)\n",
    "grid_size = (int(math.ceil(height / float(dim_size))),\n",
    "             int(math.ceil(width / float(dim_size))))\n",
    "\n",
    "matrixtranspose = mod.get_function(\"matrixtranspose\")\n",
    "start_time = time.time()\n",
    "matrixtranspose(\n",
    "        cuda.In(a),\n",
    "        cuda.Out(trA),\n",
    "        np.uint16(width),\n",
    "        np.uint16(height),\n",
    "        block=block_size,\n",
    "        grid=grid_size)\n",
    "total_time_transpose = time.time() - start_time\n",
    "\n",
    "print(trA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Shared Memory implementation\n",
    "------------------------------\n",
    "To get coalesced memory access, we will use shared memory. We will use the\n",
    "shared memory to save the part of global memory, which is read by the thread\n",
    "block. We can then use this saved to write to the correct place in another\n",
    "thread to get coalesced access.\n",
    "\n",
    "Before we go on, we have to introduce barriers. Barriers are a way to ensure\n",
    "that all threads, within a thread block, have reached a specific point before\n",
    "any can continue. This is useful especially when using larger kernels or shared\n",
    "memory, where we can make sure that every thread in the thread block has come\n",
    "beyond a specific point. In CUDA we can use a barrier by calling the\n",
    "`__syncthreads()` function. It is also important to note, that all code must\n",
    "pass the same barrier at the same time. Using barriers in a different way will\n",
    "result in undefined behaviour.\n",
    "![One thread is yet to reach the barrier, so the two others are waiting](barrier.png)\n",
    "![All threads have reached the barrier, so they now can continue](barrier.png)\n",
    "\n",
    "To get coalesced access with share memory, we need to use the `blockIdx` to move\n",
    "our thread blocks. By swapping `blockIdx.x` and `blockIdx.y`, when we calculate\n",
    "our position, we can simply transpose within the block in shared memory and\n",
    "write that result to global memory.\n",
    "![Swapping two thread blocks in a small grid](threadblocks.png)\n",
    "\n",
    "```c++\n",
    "#define T 16\n",
    "__global__ void matrixtranspose(\n",
    "    const int *A,\n",
    "    int *trA,\n",
    "    ushort colsA,\n",
    "    ushort rowsA)\n",
    "{\n",
    "    __shared__ int tile[T][T+1];\n",
    "    int tidx = threadIdx.x;\n",
    "    int tidy = threadIdx.y;\n",
    "    int i = blockIdx.x*T + tidx;\n",
    "    int j = blockIdx.y*T + tidy;\n",
    "    if(j < colsA && i < rowsA) {\n",
    "        tile[tidy][tidx] = A[i * colsA + j];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    i = blockIdx.y*T + threadIdx.x;\n",
    "    j = blockIdx.x*T + threadIdx.y;\n",
    "    if(j < colsA && i < rowsA) {\n",
    "        trA[i * rowsA + j] = tile[tidx][tidy];\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import math\n",
    "import time\n",
    "\n",
    "width = 25000\n",
    "height = 25000\n",
    "\n",
    "a = np.arange(height * width).astype(np.int32)\n",
    "a.shape = (height, width)\n",
    "\n",
    "trA = np.empty((width, height)).astype(np.int32)\n",
    "\n",
    "dim_size = 32\n",
    "block_size = (dim_size,dim_size,1)\n",
    "grid_size = (int(math.ceil(height / float(dim_size))),\n",
    "             int(math.ceil(width / float(dim_size))))\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "            #define T \"\"\" + str(dim_size) + \"\"\"\n",
    "            __global__ void matrixtranspose(\n",
    "                const int *A,\n",
    "                int *trA,\n",
    "                ushort colsA,\n",
    "                ushort rowsA)\n",
    "            {\n",
    "                __shared__ int tile[T][T+1];\n",
    "                int tidx = threadIdx.x;\n",
    "                int tidy = threadIdx.y;\n",
    "                int i = blockIdx.x*T + tidx;\n",
    "                int j = blockIdx.y*T + tidy;\n",
    "                if(j < colsA && i < rowsA) {\n",
    "                    tile[tidy][tidx] = A[j * colsA + i];\n",
    "                }\n",
    "                __syncthreads();\n",
    "                i = blockIdx.y*T + tidx;\n",
    "                j = blockIdx.x*T + tidy;\n",
    "                if(j < rowsA && i < colsA) {\n",
    "                    trA[j * rowsA + i] = tile[tidx][tidy];\n",
    "                }\n",
    "            }\n",
    "            \"\"\")\n",
    "\n",
    "matrixtranspose = mod.get_function(\"matrixtranspose\")\n",
    "start_time = time.time()\n",
    "matrixtranspose(\n",
    "        cuda.In(a),\n",
    "        cuda.Out(trA),\n",
    "        np.uint16(width),\n",
    "        np.uint16(height),\n",
    "        block=block_size,\n",
    "        grid=grid_size)\n",
    "total_time_shared = time.time() - start_time\n",
    "\n",
    "print(trA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Dynamically allocated shared memory implementation\n",
    "----------------------------------------------------\n",
    "In this implementation we will use dynamically allocated shared memory instead\n",
    "of allocating it directly in the kernel. It does not yield any specific\n",
    "performance benefit to dynamically allocate shared memory. But it will make the\n",
    "kernel more general and you will need less code to handle changing block sizes.\n",
    "We will also need to change the way we call our kernel by adding the argument\n",
    "`shared`, defining the number of bytes needed, to the function call.\n",
    "\n",
    "```c++\n",
    "__global__ void matrixtranspose(\n",
    "    const int *A,\n",
    "    int *trA,\n",
    "    ushort colsA,\n",
    "    ushort rowsA)\n",
    "{\n",
    "    extern __shared__ int tile[];\n",
    "    int sharedIdx = threadIdx.y*blockDim.y + threadIdx.x;\n",
    "    int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "    int j = blockIdx.y*blockDim.y + threadIdx.y;\n",
    "    if( j < colsA && i < rowsA ) {\n",
    "        tile[sharedIdx] = A[i * colsA + j];\n",
    "    }\n",
    "    __syncthreads();\n",
    "    i = blockIdx.y*blockDim.y + threadIdx.x;\n",
    "    j = blockIdx.x*blockDim.x + threadIdx.y;\n",
    "    if(j < colsA && i < rowsA) {\n",
    "        sharedIdx = threadIdx.x*blockDim.x + threadIdx.y;\n",
    "        trA[i * rowsA + j] = tile[sharedIdx];\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        0     25000     50000 ... 624925000 624950000 624975000]\n",
      " [        1     25001     50001 ... 624925001 624950001 624975001]\n",
      " [        2     25002     50002 ... 624925002 624950002 624975002]\n",
      " ...\n",
      " [    24997     49997     74997 ... 624949997 624974997 624999997]\n",
      " [    24998     49998     74998 ... 624949998 624974998 624999998]\n",
      " [    24999     49999     74999 ... 624949999 624974999 624999999]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import math\n",
    "import time\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "        __global__ void matrixtranspose(\n",
    "            const int *A,\n",
    "            int *trA,\n",
    "            ushort colsA,\n",
    "            ushort rowsA)\n",
    "        {\n",
    "            extern __shared__ int tile[];\n",
    "            int tidx = threadIdx.x;\n",
    "            int tidy = threadIdx.y;\n",
    "            int sharedIdx = tidy*blockDim.y + tidx;\n",
    "            int i = blockIdx.x*blockDim.x + tidx;\n",
    "            int j = blockIdx.y*blockDim.y + tidy;\n",
    "            if( j < colsA && i < rowsA ) {\n",
    "                tile[sharedIdx] = A[j * colsA + i];\n",
    "            }\n",
    "            __syncthreads();\n",
    "            i = blockIdx.y*blockDim.y + tidx;\n",
    "            j = blockIdx.x*blockDim.x + tidy;\n",
    "            if(j < rowsA && i < colsA) {\n",
    "                sharedIdx = tidx*blockDim.x + tidy;\n",
    "                trA[j * rowsA + i] = tile[sharedIdx];\n",
    "            }\n",
    "        }\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "width = 25000\n",
    "height = 25000\n",
    "\n",
    "a = np.arange(height * width).astype(np.int32)\n",
    "a.shape = (height, width)\n",
    "\n",
    "trA = np.empty((width, height)).astype(np.int32)\n",
    "\n",
    "dim_size = 16\n",
    "block_size = (dim_size,dim_size,1)\n",
    "grid_size = (int(math.ceil(height / float(dim_size))),\n",
    "             int(math.ceil(width / float(dim_size))))\n",
    "\n",
    "matrixtranspose = mod.get_function(\"matrixtranspose\")\n",
    "start_time = time.time()\n",
    "matrixtranspose(\n",
    "        cuda.In(a),\n",
    "        cuda.Out(trA),\n",
    "        np.uint16(width),\n",
    "        np.uint16(height),\n",
    "        block=block_size,\n",
    "        grid=grid_size,\n",
    "        shared=dim_size**2*4)\n",
    "total_time_dyn_shared = time.time() - start_time\n",
    "\n",
    "print(trA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_time_transpose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0d11c7039ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"No shared\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shared\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dynamic shared\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtotal_time_transpose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_time_shared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_time_dyn_shared\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_time_transpose' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "names = [\"No shared\", \"Shared\", \"Dynamic shared\"]\n",
    "values = [total_time_transpose, total_time_shared, total_time_dyn_shared]\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.title(\"Timing\")\n",
    "plt.bar(names, values)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
